name: Generate sitemap & robots

on:
  workflow_dispatch:

jobs:
  gen:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Build sitemap.xml and robots.txt
        shell: bash
        run: |
          set -euo pipefail
          BASE="https://mojaf-sa.com"
          TODAY="$(date +%F)"

          TMP_URLS="$(mktemp)"

          # Find all .html files, skip non-indexable folders/files
          find . \
            -type d \( -path './.git' -o -path './.github' -o -path './node_modules' -o -path './_turbo_en' -o -path './blogs' \) -prune -false -o \
            -type f -name '*.html' | while read -r f; do
              f="${f#./}"
              bn="$(basename "$f")"

              # Skip junk pages
              if [ "$f" = "404.html" ] || [ "$f" = "header-footer-snippet.html" ]; then
                continue
              fi

              # Route style: folder index → /dir/, file page → /file.html
              if [ "$bn" = "index.html" ]; then
                d="$(dirname "$f")"
                if [ "$d" = "." ]; then
                  u="$BASE/"
                else
                  u="$BASE/$d/"
                fi
              else
                u="$BASE/$f"
              fi

              # Normalize blog URL to one canonical: /blog/
              if [ "$u" = "$BASE/blog" ] || [ "$u" = "$BASE/blog.html" ]; then
                u="$BASE/blog/"
              fi

              # Normalize accidental double slashes
              u="${u//\/\//\/}"; u="${u/:\//:\/\/}"

              echo "$u" >> "$TMP_URLS"
            done

          sort -u "$TMP_URLS" > "$TMP_URLS.sorted"

          # Write sitemap.xml
          {
            echo '<?xml version="1.0" encoding="UTF-8"?>'
            echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
            while read -r u; do
              echo '  <url>'
              echo "    <loc>${u}</loc>"
              echo "    <lastmod>${TODAY}</lastmod>"
              echo '  </url>'
            done < "$TMP_URLS.sorted"
            echo '</urlset>'
          } > sitemap.xml

          # Write robots.txt with absolute Sitemap
          {
            echo 'User-agent: *'
            echo 'Allow: /'
            echo
            echo "Sitemap: ${BASE}/sitemap.xml"
          } > robots.txt

      - name: Commit & push
        shell: bash
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add sitemap.xml robots.txt
            git commit -m "chore: regenerate sitemap & robots from repo structure"
            git push
          else
            echo "No changes to commit"
          fi
